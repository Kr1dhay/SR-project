{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85b79abe-5496-49b3-ac85-b3e2847a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fae2a271-ec73-4be9-b4f1-92dcef63e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilirubin (Total)             float64\n",
      "Albumin                       float64\n",
      "INR                           float64\n",
      "Ascites                         int64\n",
      "Hepatic encephalopathy          int64\n",
      "Re-Bleeding within 14 days      int64\n",
      "dtype: object\n",
      "Length of Dataframe: 913\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"childpugh.csv\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"Length of Dataframe: {len(df)}\")\n",
    "\n",
    "EXPERIMENTS_START_SEED = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63a93524-c49a-4c1a-84cb-d90142f7d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 5, Max: 12, Mean: 7.62, Median: 8.00\n",
      "count    913.000000\n",
      "mean       7.616648\n",
      "std        1.431103\n",
      "min        5.000000\n",
      "25%        6.000000\n",
      "50%        8.000000\n",
      "75%        9.000000\n",
      "max       12.000000\n",
      "Name: child_pugh_score, dtype: float64\n",
      "child_pugh_score\n",
      "5      52\n",
      "6     189\n",
      "7     195\n",
      "8     185\n",
      "9     219\n",
      "10     60\n",
      "11     12\n",
      "12      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def calculate_child_pugh_score(row):\n",
    "    \"\"\"\n",
    "    Calculate Child-Pugh score based on the 5 parameters.\n",
    "    Returns total score (5-15 points).\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # 1. Bilirubin (Total) in mg/dL\n",
    "    bilirubin = row['Bilirubin (Total)']\n",
    "    if bilirubin < 2:\n",
    "        score += 1\n",
    "    elif 2 <= bilirubin <= 3:\n",
    "        score += 2\n",
    "    else:  # > 3\n",
    "        score += 3\n",
    "    \n",
    "    # 2. Albumin in g/dL\n",
    "    albumin = row['Albumin']\n",
    "    if albumin > 3.5:\n",
    "        score += 1\n",
    "    elif 2.8 <= albumin <= 3.5:\n",
    "        score += 2\n",
    "    else:  # < 2.8\n",
    "        score += 3\n",
    "    \n",
    "    # 3. INR\n",
    "    inr = row['INR']\n",
    "    if inr < 1.7:\n",
    "        score += 1\n",
    "    elif 1.7 <= inr <= 2.3:\n",
    "        score += 2\n",
    "    else:  # > 2.3\n",
    "        score += 3\n",
    "    \n",
    "    # 4. Ascites\n",
    "    # Assuming: 0=None(1pt), 1=Mild(2pt), 2=Severe(3pt)\n",
    "    ascites = row['Ascites']\n",
    "    score += (ascites + 1)\n",
    "    \n",
    "    # 5. Hepatic Encephalopathy\n",
    "    # Assuming: 0=None(1pt), 1=Grade1-2(2pt), 2=Grade3-4(3pt)\n",
    "    hepatic_enc = row['Hepatic encephalopathy']\n",
    "    score += (hepatic_enc + 1)\n",
    "    \n",
    "    return int(score)\n",
    "\n",
    "df[\"child_pugh_score\"] = df.apply(calculate_child_pugh_score, axis=1)\n",
    "\n",
    "print(f\"Min: {df['child_pugh_score'].min()}, Max: {df['child_pugh_score'].max()}, Mean: {df['child_pugh_score'].mean():.2f}, Median: {df['child_pugh_score'].median():.2f}\")\n",
    "print(df['child_pugh_score'].describe())\n",
    "print(df['child_pugh_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54eb5c7a-07c1-4e67-a28e-c20c3dc8dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 20/100 iterations...\n",
      "Completed 40/100 iterations...\n",
      "Completed 60/100 iterations...\n",
      "Completed 80/100 iterations...\n",
      "Completed 100/100 iterations...\n",
      "\n",
      "Child-Pugh Score Bootstrap Results (100 iterations):\n",
      "============================================================\n",
      "            mean     std     min     max  95%_CI_lower  95%_CI_upper\n",
      "auroc     0.5737  0.0622  0.4153  0.7668        0.4681        0.6836\n",
      "accuracy  0.5626  0.0919  0.3102  0.7153        0.4635        0.7099\n",
      "f1        0.1701  0.0322  0.0588  0.2500        0.1009        0.2352\n",
      "\n",
      "\n",
      "Detailed metrics:\n",
      "AUROC:    0.5737 ± 0.0622 [0.4681, 0.6836]\n",
      "Accuracy: 0.5626 ± 0.0919 [0.4635, 0.7099]\n",
      "F1 Score: 0.1701 ± 0.0322 [0.1009, 0.2352]\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics are calculated using 100 samples \n",
    "def bootstrap_model_evaluation(df, model, random_state_start, n_iterations=100, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Bootstrap train/test splits with different seeds and evaluate model performance.\n",
    "    Optimal threshold selected using Youden's J statistic on train set.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pandas.DataFrame\n",
    "        AUROC, Accuracy, F1 for each iteration\n",
    "    summary_stats : pandas.DataFrame\n",
    "        Mean, std, and 95% CI for each metric\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare features and target\n",
    "    Y_score = df['child_pugh_score']\n",
    "    Y = df['Re-Bleeding within 14 days']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        seed = random_state_start + i\n",
    "        \n",
    "        # Create train/test split\n",
    "        y_score_train, y_score_test, y_train, y_test = train_test_split(\n",
    "            Y_score, Y, test_size=test_size, random_state=seed, stratify=Y\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Find optimal threshold using Youden's J statistic on TRAIN set\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "        youden_j = tpr_train - fpr_train\n",
    "        optimal_idx = np.argmax(youden_j)\n",
    "        optimal_threshold = thresholds_train[optimal_idx]\n",
    "        \n",
    "        # Apply optimal threshold to TEST set\n",
    "        y_test_pred = (y_score_test >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics on TEST set\n",
    "        metrics = {\n",
    "            'seed': seed,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'auroc': roc_auc_score(y_test, y_score_test),\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'f1': f1_score(y_test, y_test_pred)\n",
    "        }\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Completed {i + 1}/{n_iterations} iterations...\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    metric_cols = ['auroc', 'accuracy', 'f1']\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'mean': results_df[metric_cols].mean(),\n",
    "        'std': results_df[metric_cols].std(),\n",
    "        'min': results_df[metric_cols].min(),\n",
    "        'max': results_df[metric_cols].max(),\n",
    "        '95%_CI_lower': results_df[metric_cols].quantile(0.025),\n",
    "        '95%_CI_upper': results_df[metric_cols].quantile(0.975)\n",
    "    })\n",
    "    \n",
    "    return results_df, summary_stats\n",
    "\n",
    "\n",
    "# Call the function with your dataset\n",
    "results_df, summary_stats = bootstrap_model_evaluation(df, model=None, random_state_start=EXPERIMENTS_START_SEED)\n",
    "\n",
    "print(\"\\nChild-Pugh Score Bootstrap Results (100 iterations):\")\n",
    "print(\"=\"*60)\n",
    "print(summary_stats.round(4))\n",
    "print(\"\\n\")\n",
    "print(\"Detailed metrics:\")\n",
    "print(f\"AUROC:    {summary_stats.loc['auroc', 'mean']:.4f} ± {summary_stats.loc['auroc', 'std']:.4f} \"\n",
    "      f\"[{summary_stats.loc['auroc', '95%_CI_lower']:.4f}, {summary_stats.loc['auroc', '95%_CI_upper']:.4f}]\")\n",
    "print(f\"Accuracy: {summary_stats.loc['accuracy', 'mean']:.4f} ± {summary_stats.loc['accuracy', 'std']:.4f} \"\n",
    "      f\"[{summary_stats.loc['accuracy', '95%_CI_lower']:.4f}, {summary_stats.loc['accuracy', '95%_CI_upper']:.4f}]\")\n",
    "print(f\"F1 Score: {summary_stats.loc['f1', 'mean']:.4f} ± {summary_stats.loc['f1', 'std']:.4f} \"\n",
    "      f\"[{summary_stats.loc['f1', '95%_CI_lower']:.4f}, {summary_stats.loc['f1', '95%_CI_upper']:.4f}]\")\n",
    "\n",
    "#Save Results\n",
    "results_df.to_csv('child_pugh_bootstrap_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dc0c9-3c6f-47fc-827b-7ac6bf712bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40c8ae-5825-4f4a-a29f-65705ffea12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
